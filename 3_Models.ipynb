{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY6UfdW5XjA_"
   },
   "source": [
    "## 전체적인 구현 과정 및 분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVDh9DxbpGM3"
   },
   "source": [
    "**1. 데이터 프로세싱 방법** <br>\n",
    "  1.1 영어 & 한국어 데이터 전처리\n",
    "- 영어 데이터: NLTK의 BracketParseCorpusReader 모듈을 통해 처리\n",
    "- 한국어 데이터: 구문표지를 바탕으로 어순을 정렬한 후 형태소만 추출. 'NP_SBJ(주격 체언구)' 뒤에 위치하는 형태소들의 순서를 뒤집는 방식으로 어순 정렬\n",
    "\n",
    "1.2 영어-한국어 parallel 데이터쌍을 만든 후 shuffle. \n",
    "\n",
    "**2. Embedding별 성능 비교** <br>\n",
    "Static embedding을 필요로 하는 packed-padded encoder decoder model과 convolutional sequence to sequence model 둘 다 한국어 데이터는 word2vec 사전학습 임베딩, 영어 데이터는 fasttext 사전학습 임베딩을 썼을 때 제일 성능이 높았다.\n",
    "- 한국어 word2vec 사전학습 임베딩: 중간과제 자료 그대로 활용\n",
    "- 영어 fasttext 사전학습 임베딩: torchtext 기본제공 임베딩 활용\n",
    "\n",
    "**3. Model별 Hyperparameter** <br>\n",
    "3.1. packed-padded encoder decoder model\n",
    "- Batch size: 128\n",
    "- Encpder, decoder dropout rate: 0.5\n",
    "- Teacher forcing rate: 0.5\n",
    "- Learning rate(Adam Optimizer): 0.001\n",
    "- Epoch size: 5\n",
    "- Clip: 1 <br>\n",
    "\n",
    "epoch size를 5 이상 늘리면 overfitting하였고, teacher forcing rate를 늘리자 perplexity와 BLEU score가 모두 증가하는 trade-off가 발생하였다. Clip을 줄이자 epoch5일 때 perplexity의 발산이 발생하였다.\n",
    "\n",
    "3.2. convolutional sequence to sequence model\n",
    "- Batch size: 128\n",
    "- Encoder, decoder dropout rate: 0.25\n",
    "- Learning rate(Adam Optimizer): 0.001\n",
    "- Epoch size: 5\n",
    "- Clip: 1 <br>\n",
    "\n",
    "epoch size를 5 이상 늘리면 overfitting하였고, dropout rate를 0.3으로 늘리자 perplexity가 가하고 BLEU score가 감소하였으며 그 이상으로 dropout rate를 늘리면 perplexity가 발산하였다. Clip을 줄이자 epoch5일 때 perplexity의 발산이 발생하였다.\n",
    "\n",
    "3.3. transformers from scratch\n",
    "- Batch size: 128\n",
    "- Encoder, decoder dropout rate: 0.1\n",
    "- Encoder, decoder layers: 3 each\n",
    "- Encoder, decoder heads: 8 each\n",
    "- Dimensionality of the layers and the pooler layer: 512\n",
    "- Learning rate: 0.0005\n",
    "\n",
    "**4. Train, Validation, Test 결과** <br>\n",
    "4.1. packed-padded encoder decoder model\n",
    "- Train perplexity: 4.042\n",
    "- Validation perplexity: 10.193\n",
    "- Test perplexity: 10.117\n",
    "\n",
    "4.2. convolutional sequence to sequence model\n",
    "- Train perplexity: 5.053\n",
    "- Validation perplexity: 3.858\n",
    "- Test perplexity: 3.803\n",
    "\n",
    "4.3. transformers from scratch\n",
    "- Train perplexity: 2.596\n",
    "- Validation perplexity: 2.746\n",
    "- Test perplexity: 2.770\n",
    "\n",
    "**5. BLEU score** <br>\n",
    "5.1 packed-padded encoder decoder model: 42.38 <br>\n",
    "5.2 convolutional sequence to sequence model: 38.11 <br>\n",
    "5.3 transformers from scratch: 43.92 <br>\n",
    "\n",
    "**6. 모델 성능 비교**\n",
    "- 세 모델 중에는 test perplexity와 BLEU score 모두 transformers가 우수하였다. \n",
    "- inference 시 user input의 어순을 조작하지 않고 그대로 넣는 것이 성능이 더 좋았다. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25546,
     "status": "ok",
     "timestamp": 1607946253369,
     "user": {
      "displayName": "­박재은 / 학생 / 심리학과",
      "photoUrl": "",
      "userId": "00567750484884375615"
     },
     "user_tz": -540
    },
    "id": "VYCT5utw6YrG",
    "outputId": "acc25471-8d5c-4532-c298-d77d7c07f0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiNnN6_g3pnR"
   },
   "source": [
    "### 1. 필요 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2YO3qigapsA",
    "outputId": "ce8ddc93-71fb-4865-ae7a-9d2b2ef0526d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/pytorch/text\n",
      "  Cloning https://github.com/pytorch/text to /tmp/pip-req-build-uslohhm4\n",
      "  Running command git clone -q https://github.com/pytorch/text /tmp/pip-req-build-uslohhm4\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+ec413ff) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+ec413ff) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+ec413ff) (1.7.0+cu101)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+ec413ff) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+ec413ff) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+ec413ff) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+ec413ff) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+ec413ff) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.9.0a0+ec413ff) (0.8)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.9.0a0+ec413ff) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.9.0a0+ec413ff) (3.7.4.3)\n",
      "Building wheels for collected packages: torchtext\n",
      "  Building wheel for torchtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torchtext: filename=torchtext-0.9.0a0+ec413ff-cp36-cp36m-linux_x86_64.whl size=7140989 sha256=92a429a4dc1447616e1ebc3aac42eb77ada34adcb8c25ae0a7205e41a68bc4f6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3ixnjozb/wheels/73/14/71/ed033fd999ae4933e17df3e91be2014e61c2f312a88a164ff5\n",
      "Successfully built torchtext\n",
      "Installing collected packages: torchtext\n",
      "  Found existing installation: torchtext 0.9.0a0+ec413ff\n",
      "    Uninstalling torchtext-0.9.0a0+ec413ff:\n",
      "      Successfully uninstalled torchtext-0.9.0a0+ec413ff\n",
      "Successfully installed torchtext-0.9.0a0+ec413ff\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/pytorch/text #upgrading torchtext for colab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import TranslationDataset\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGXZAREJFP3-"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-3_Z3-rhXOY"
   },
   "source": [
    "### 2. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 한국어 데이터: 영어 어순에 맞게 어순을 reverse하는 과정 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KCFaUzEZ06R"
   },
   "outputs": [],
   "source": [
    "#한국어 데이터 로드 및 정규식 이용하여 프로세싱\n",
    "ko_path = \"/content/drive/MyDrive/dataset/ko-en.ko.parse\"\n",
    "\n",
    "with open (ko_path, 'r', encoding='utf-8') as f:\n",
    "  data = f.read()\n",
    "  contents = re.findall('<id.*?/id>', data, re.S)\n",
    "  sentences = []\n",
    "  for c in contents:\n",
    "    pattern = '<.+?>'\n",
    "    sent = re.sub(pattern, '', c)\n",
    "    sent = re.sub('\\n', '\\t', sent)\n",
    "    sentences.append(sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vJiBfrZyb1d"
   },
   "outputs": [],
   "source": [
    "#한국어 데이터에서 탭으로 구분된 각 column을 분리 + 한 id에 sent가 2개 이상인 경우 공백 제거\n",
    "tabs_rm_b = []\n",
    "\n",
    "for sents in sentences:\n",
    "\n",
    "  split_sent = sents.split('\\t')\n",
    "\n",
    "  #공백이 있을 경우 공백 제거\n",
    "  if '' in split_sent: \n",
    "    while '' in split_sent:\n",
    "      split_sent.remove('') \n",
    "\n",
    "  tabs_rm_b.append(split_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSQtrjxvHRFU"
   },
   "outputs": [],
   "source": [
    "#데이터프레임으로 변경\n",
    "ko_df = pd.DataFrame(tabs_rm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDBd4ljqVD9l"
   },
   "outputs": [],
   "source": [
    "#한국어 데이터에서 세 번째 열(구문 표지)과 네 번째 열(형태소와 품사정보) 각각 추출하여 튜플의 형태로 만듬 (구문표지, 형태소와 품사정보)\n",
    "ko_corpus_list = []\n",
    "\n",
    "for i in range (0, len(ko_df.index)):\n",
    "  row = ko_df.loc[i,:].dropna()\n",
    "  length = len(row)\n",
    "\n",
    "  j = 2\n",
    "\n",
    "  ko_sen_list = []\n",
    "\n",
    "  while j <= length-1:\n",
    "    index = row[j]\n",
    "    word = row[j+1]\n",
    "\n",
    "    ko_tup = (index,word)\n",
    "\n",
    "    ko_sen_list.append(ko_tup)\n",
    "\n",
    "    j = j + 4\n",
    "\n",
    "  ko_corpus_list.append(ko_sen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5fP16GtjXQ_"
   },
   "outputs": [],
   "source": [
    "#한국어 각 문장의 어순을 바꿈. NP_SUB(주격 체언구)가 있는 경우 NP_SUB 뒤의 형태소들을 reverse. 없는 경우는 전체 형태소들을 reverse\n",
    "ordered_ko_corpus = []\n",
    "\n",
    "for i in range(0, corpus_len):\n",
    "  sen = ko_corpus_list[i]\n",
    "  sen_mi_list = []\n",
    "  \n",
    "  for n in range(0, len(sen)):\n",
    "    sen_morph_index = sen[n][1]\n",
    "    sen_mi_list.append(sen_morph_index)\n",
    "\n",
    "  #문장이 2개 이상인 경우, 품사 정보 중 SF(마침표, 물음표, 느낌표)를 기준으로 문장 분리 \n",
    "\n",
    "  sf_check = list(filter(lambda x: 'SF' in x, sen_mi_list)) #ref: https://coding-groot.tistory.com/21\n",
    "\n",
    "  if len(sf_check) >= 2: \n",
    "\n",
    "    sf_index = []\n",
    "\n",
    "    for ind in sf_check:\n",
    "      index = sen_mi_list.index(ind)\n",
    "      sf_index.append(index)\n",
    "\n",
    "    multi_sen = []\n",
    "\n",
    "    a = 0\n",
    "\n",
    "    for sf_i in sf_index:\n",
    "      sen_index_list = []\n",
    "\n",
    "      raw_multi_sen = [word for word in sen[a:sf_i+1]]\n",
    "\n",
    "      for j in range(0, len(sen)):\n",
    "        sen_index = sen[j][0]\n",
    "        sen_index_list.append(sen_index)\n",
    "\n",
    "      if 'NP_SBJ' in sen_index_list:\n",
    "        ns_index = sen_index_list.index('NP_SBJ')\n",
    "\n",
    "        raw_sen = [word for word in raw_multi_sen[:ns_index+1]]\n",
    "\n",
    "        reverse_sen = [word for word in raw_multi_sen[ns_index+1:]]\n",
    "        reverse_sen = reverse_sen[::-1]\n",
    "\n",
    "        new_sen = raw_sen + reverse_sen\n",
    "\n",
    "      else:\n",
    "        new_sen = raw_multi_sen[::-1]\n",
    "      \n",
    "      multi_sen.append(new_sen)\n",
    "\n",
    "      a = a+sf_i+1\n",
    "    \n",
    "    new_sen = [item for uni in multi_sen for item in uni]\n",
    "\n",
    "  else: \n",
    "    sen_index_list = []\n",
    "    \n",
    "    for j in range(0, len(sen)):\n",
    "      sen_index = sen[j][0]\n",
    "      sen_index_list.append(sen_index)\n",
    "      \n",
    "    if 'NP_SBJ' in sen_index_list:\n",
    "      ns_index = sen_index_list.index('NP_SBJ')\n",
    "\n",
    "      raw_sen = [word for word in sen[:ns_index+1]]\n",
    "\n",
    "      reverse_sen = [word for word in sen[ns_index+1:]]\n",
    "      reverse_sen = reverse_sen[::-1]\n",
    "\n",
    "      new_sen = raw_sen + reverse_sen\n",
    "\n",
    "    else:\n",
    "      new_sen = sen[::-1]\n",
    "  \n",
    "  ordered_ko_corpus.append(new_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jncSM55YuXrO"
   },
   "outputs": [],
   "source": [
    "#어순을 조정한 데이터들에 대해 품사정보 표지를 제거하고 형태소만 남김\n",
    "clean_ko_corpus = []\n",
    "\n",
    "for sen in ordered_ko_corpus:\n",
    "\n",
    "  clean_1 = []\n",
    "  clean_2 = []\n",
    "\n",
    "  for i in range(0, len(sen)):\n",
    "    word = sen[i][1]\n",
    "\n",
    "    if '|' in word:\n",
    "      new_word = word.split('|')\n",
    "      for w in new_word:\n",
    "        clean_1.append(w)\n",
    "\n",
    "    else:\n",
    "      clean_1.append(word)\n",
    "\n",
    "  \n",
    "  for token in clean_1:\n",
    "    new_token = token.split('/')\n",
    "    clean_2.append(new_token[0])\n",
    "    \n",
    "    clean_sen = \" \".join(clean_2)\n",
    "  \n",
    "  clean_ko_corpus.append(clean_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 영어 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnS_TrSi6oex"
   },
   "outputs": [],
   "source": [
    "#영어 데이터 프로세싱을 위해 nltk의 BracketParseCorpusReader 모듈 사용 \n",
    "#BracketParseCorpusReader를 통해 영어 데이터의 문장, 단어, 품사태깅된 문장 등을 불러올 수 있음\n",
    "\n",
    "from nltk.corpus.reader import BracketParseCorpusReader\n",
    "en = BracketParseCorpusReader(root=\"/content/drive/MyDrive/dataset/\", fileids=['ko-en.en.parse.syn'], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvzdbHaPAuFA",
    "outputId": "a6256c80-cc1d-40a5-8887-08410a41c074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330974\n",
      "330974\n"
     ]
    }
   ],
   "source": [
    "#한국어 데이터와 영어 데이터 개수 비교, 동일함을 확인\n",
    "print(len(clean_ko_corpus))\n",
    "print(len(en.tagged_sents(fileids='ko-en.en.parse.syn'))) #tagged_sent 통해 단어와 품사를 함께 불러올 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGy5yxXEEFkP"
   },
   "outputs": [],
   "source": [
    "#각 영어 문장별 형태소 리스트 생성 \n",
    "en_word_list = list(en.sents(fileids='ko-en.en.parse.syn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 영어 문장별로 각 형태소가 공백으로 나뉜 텍스트 생성\n",
    "tokenized_sentences = [\" \".join(sent) for sent in en_word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 한국어-영어 문장이 짝지어진 데이터셋 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qfeSNVXGG1S"
   },
   "outputs": [],
   "source": [
    "#(영어) 각 문장이 newline token으로 나뉜 텍스트 파일 생성 \n",
    "f = open('/content/drive/MyDrive/dataset/en.txt', mode='wt', encoding='utf-8')\n",
    "for sent in tokenized_sentences:\n",
    "  f.write(sent)\n",
    "  f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R8CazorGfDW"
   },
   "outputs": [],
   "source": [
    "#(한국어) 각 문장이 newline token으로 나뉜 텍스트 파일 생성 \n",
    "f = open('/content/drive/MyDrive/dataset/ko.txt', mode='wt', encoding='utf-8')\n",
    "for sent in clean_ko_corpus:\n",
    "  f.write(sent)\n",
    "  f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tI5BnBaPWMY",
    "outputId": "f59a0d03-7c56-4a39-ee81-a199ab8673d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#데이터 셔플링을 위해 DataFrame 이용 \n",
    "\n",
    "df1 = pd.read_csv('/content/drive/MyDrive/dataset/ko.txt', sep='/n,', names=['src'], header=None) # 한국어\n",
    "df2 = pd.read_csv('/content/drive/MyDrive/dataset/en.txt', sep='/n,', names=['trg'], header=None) # 영어\n",
    "\n",
    "df = pd.concat([df1,df2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 셔플\n",
    "df_shuffle = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKIRnCryQPJI"
   },
   "outputs": [],
   "source": [
    "df_src = df_shuffle['src']\n",
    "df_trg = df_shuffle['trg']\n",
    "\n",
    "df_src.to_csv('/content/drive/MyDrive/dataset/ko_shuffle.txt', sep = '\\n', index = False, header=None)\n",
    "df_trg.to_csv('/content/drive/MyDrive/dataset/en_shuffle.txt', sep = '\\n', index = False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-0sbxIX2Zeb"
   },
   "source": [
    "### 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Packed Encoder-Decoder\n",
    "Reference: https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR0aBWzetM7M"
   },
   "source": [
    "### (1) Field 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FE6XuILFJsrd"
   },
   "outputs": [],
   "source": [
    "#위에서 한국어와 영어 문장의 형태소가 공백으로 분리된 텍스트 파일을 생성\n",
    "#형태소를 그대로 추출하기 위해 tokenizer 함수로 whitespace 기준 split함수 사용  \n",
    "\n",
    "def tokenize_ko(text):\n",
    "\n",
    "    return [tok for tok in text.split(\" \")]\n",
    "\n",
    "def tokenize_en(text):\n",
    "\n",
    "    return [tok for tok in text.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISM0kERkJYpb",
    "outputId": "c6c3cdee-b7a6-4216-fc99-ceb27d3b4b3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#Field 정의\n",
    "#Packed Encoder-Decoder Model 사용 위해 출발어(SRC) 문장 길이 정보 사용 \n",
    "\n",
    "SRC = Field(tokenize = tokenize_ko, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            include_lengths = True,\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK79VGjEvD1f"
   },
   "source": [
    "### (2) TranslationDataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CTgF5qGQ4-E",
    "outputId": "fa3acbb5-3fc5-4ffa-baad-e97bd3800fad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#셔플된 데이터를 torchtext의 TranslationDataset을 통해 불러옴\n",
    "\n",
    "data_shuffled = TranslationDataset(path = '/content/drive/MyDrive/dataset/ordered_data/', exts = ('ko_shuffle.txt', 'en_shuffle.txt'), fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2RCMjtFWw23"
   },
   "source": [
    "### (3) Train, Validation, Test Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V29SA7SaMjYF"
   },
   "outputs": [],
   "source": [
    "#train, validation, test dataset 각각 80:10:10 비율로 생성 \n",
    "train_data, test_data = data_shuffled.split(split_ratio = 0.8, random_state = random.seed(SEED))\n",
    "valid_data, test_data = test_data.split(split_ratio = 0.5, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5aycqr2M_xu",
    "outputId": "1a3351ac-31e8-458f-9eb5-041ddd7c5b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 264779\n",
      "Number of validation examples: 33098\n",
      "Number of testing examples: 33097\n"
     ]
    }
   ],
   "source": [
    "#train, validation, test data 개수 확인 \n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D88HRMw_YU2a"
   },
   "source": [
    "### (4) 한국어, 영어 vocab 생성 및 사전학습 임베딩값 설정\n",
    "Reference: https://rohit-agrawal.medium.com/using-fine-tuned-gensim-word2vec-embeddings-with-torchtext-and-pytorch-17eea2883cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_fWi-9rRoDb"
   },
   "outputs": [],
   "source": [
    "#영어 임베딩으로 torchtext에서 제공하는 fasttext.en.300d 모델을 사용 \n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, vectors='fasttext.en.300d', min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cAsVLKsT2hQ",
    "outputId": "8f4f8dc5-a526-4b81-d3a3-5f407cce871d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ko) vocabulary: 16179\n",
      "Unique tokens in target (en) vocabulary: 14245\n"
     ]
    }
   ],
   "source": [
    "#한국어와 영어 vocabulary 개수 \n",
    "print(f\"Unique tokens in source (ko) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVPCVazSSrbc"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "path = '/content/drive/MyDrive/model/'\n",
    "Word2Vec_300D_token_model = KeyedVectors.load_word2vec_format(path + 'Word2Vec_300D_token.model', binary=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "df37647370c24c62b9639aeba45d7c2c",
      "912fd8ac30e64ca7904a0d847952af40",
      "680afb4881d54c6ab138e98ad45a82e3",
      "64a2cd866508454a84cd6d416ac3e0b4",
      "882acbc01eed4a10aeb7d4ad7866b527",
      "b48d528f62e74f51a359232518198b3b",
      "39c2244510d64000803b98cb800c636e",
      "7480b0b20d554134948ee2e3e9512f87"
     ]
    },
    "id": "mXapRdw35r-S",
    "outputId": "43e3fdc7-d76c-4b38-eb47-aff44517e2ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df37647370c24c62b9639aeba45d7c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16179.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec_vectors_src = []\n",
    "\n",
    "for token, idx in tqdm_notebook(SRC.vocab.stoi.items()):\n",
    "    if token in Word2Vec_300D_token_model.wv.vocab.keys(): #사전학습 임베딩 모델에 해당 토큰의 임베딩 값이 있을 경우 그 값을 가져옴\n",
    "        word2vec_vectors_src.append(torch.FloatTensor(Word2Vec_300D_token_model[token]))\n",
    "    else:\n",
    "        word2vec_vectors_src.append(torch.randn(300)) #사전학습 임베딩 모델에 임베딩 값이 없을 경우 랜덤으로 설정\n",
    "        \n",
    "SRC.vocab.set_vectors(SRC.vocab.stoi, word2vec_vectors_src, 300) #Vocab 각 토큰의 임베딩 값 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDCbo6LFZs1z"
   },
   "source": [
    "### (5) BucketIterator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGU8MOGQU_g2"
   },
   "outputs": [],
   "source": [
    "#device 정의 \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uPrM_WQsUzwH",
    "outputId": "98035bc3-d005-4525-9efd-9d26348ed851"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#BucketIterator 생성 \n",
    "#source sentence를 길이에 따라 정렬 \n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     sort_within_batch = True,\n",
    "     sort_key = lambda x : len(x.src),\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xliZr485aB42"
   },
   "source": [
    "### (6) Packed Padded Encoder-Decoder Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrNip2yTZ3Ck"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.cpu())\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5GStXlbaTOo"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention = [batch size, src len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdBUB9HJaXFT"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sP0oqJanadv1"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rJnKx-pb9YB"
   },
   "source": [
    "### (7) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88agKk_za0kJ"
   },
   "outputs": [],
   "source": [
    "#hyperparameters 설정\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 300\n",
    "DEC_EMB_DIM = 300\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "\n",
    "#모델에 hyperparameters 입력\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOcLercTa_uv",
    "outputId": "20a1a28e-c0a1-4a22-f438-33e9d2027631"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(16179, 300)\n",
       "    (rnn): GRU(300, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(14245, 300)\n",
       "    (rnn): GRU(1324, 512)\n",
       "    (fc_out): Linear(in_features=1836, out_features=14245, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 파라미터 초기화 \n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c9ckyxOfoRl",
    "outputId": "22f0b4b1-1062-4427-a12e-c0454b8e89a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.1093, -0.1233, -0.2521,  ...,  0.3467,  0.1614,  0.1974],\n",
       "        [ 0.0728, -0.2033, -0.0522,  ...,  0.2844,  0.0839, -0.2510],\n",
       "        [-0.0395, -0.2716, -0.0760,  ...,  0.5530, -0.0286,  0.1793]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#사전학습 임베딩값을 가져옴  \n",
    "pretrained_embeddings_src = SRC.vocab.vectors\n",
    "model.encoder.embedding.weight.data.copy_(pretrained_embeddings_src)\n",
    "\n",
    "pretrained_embeddings_trg = TRG.vocab.vectors\n",
    "model.decoder.embedding.weight.data.copy_(pretrained_embeddings_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVBZrW9obLYf",
    "outputId": "b0abf7da-da6f-4a5d-ce36-94a44c1596d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 41,931,297 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "#모델에 쓰인 파라미터 개수 \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60JgoteybQqt"
   },
   "outputs": [],
   "source": [
    "#Adam optimizer 사용 \n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdDHeltwbRxx"
   },
   "outputs": [],
   "source": [
    "#Cross entropy loss 함수 사용 \n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #<pad> token은 loss 계산 시 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSuZcxVnbU-_"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, src_len = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_len, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlRTcReubYWB"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src, src_len = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
    "            \n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISjnb4bLbbeh"
   },
   "outputs": [],
   "source": [
    "#epoch 당 소요시간 계산 \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wgkKmY_KbeFV",
    "outputId": "9f378ec9-3b1b-4daf-e258-1ce7d93df70d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 10m 9s\n",
      "\tTrain Loss: 3.018 | Train PPL:  20.450\n",
      "\t Val. Loss: 2.777 |  Val. PPL:  16.069\n",
      "Epoch: 02 | Time: 10m 20s\n",
      "\tTrain Loss: 1.952 | Train PPL:   7.041\n",
      "\t Val. Loss: 2.475 |  Val. PPL:  11.880\n",
      "Epoch: 03 | Time: 10m 18s\n",
      "\tTrain Loss: 1.657 | Train PPL:   5.243\n",
      "\t Val. Loss: 2.371 |  Val. PPL:  10.706\n",
      "Epoch: 04 | Time: 10m 18s\n",
      "\tTrain Loss: 1.500 | Train PPL:   4.482\n",
      "\t Val. Loss: 2.355 |  Val. PPL:  10.541\n",
      "Epoch: 05 | Time: 10m 21s\n",
      "\tTrain Loss: 1.397 | Train PPL:   4.042\n",
      "\t Val. Loss: 2.322 |  Val. PPL:  10.193\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '/content/drive/MyDrive/dataset/packed-word2vec-fasttexten-v2_ordered_data.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CpKklUpieXkJ",
    "outputId": "00711665-88a3-4a77-ac30-3ae0b1e5a950"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.314 | Test PPL:  10.117 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/content/drive/MyDrive/dataset/packed-word2vec-fasttexten-v2_ordered_data.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgBL4p3IdFSx"
   },
   "source": [
    "### (9) BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmTvHmksy1E1",
    "outputId": "6a933dc2-781e-4bfe-ffd8-5d791c899048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: korean_romanizer in /usr/local/lib/python3.6/dist-packages (0.11)\n"
     ]
    }
   ],
   "source": [
    "#ref: https://github.com/osori/korean-romanizer\n",
    "!pip install korean_romanizer\n",
    "from korean_romanizer.romanizer import Romanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOmKuFXUJISM"
   },
   "outputs": [],
   "source": [
    "# input에 대하여 번역된 문장을 리턴하는 함수 \n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, reverse = False, romanize = False, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, list):                  \n",
    "        tokens = sentence\n",
    "    elif isinstance(sentence, str):                      # input이 tokenize되어있지 않을 경우 tokenize 시행 \n",
    "      if reverse == False:                               \n",
    "        tokens = [tok for tok in sentence.split(\" \")]\n",
    "      elif reverse == True:                              # input 문장을 역순으로 넣어 번역할 경우 \n",
    "        tokens = [tok for tok in sentence.split(\" \")][::-1]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tokens = [src_field.vocab.itos[i] for i in src_indexes]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
    "\n",
    "    mask = model.create_mask(src_tensor)\n",
    "        \n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "\n",
    "        attentions[i] = attention\n",
    "            \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    if romanize == False:                                           # <unk> 토큰을 치환하지 않음  \n",
    "      trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    elif romanize == True:                                          # <unk> 토큰에 대해 로마자화 \n",
    "      trg_tokens = []\n",
    "\n",
    "      attention_for_alignment = attentions[:len(trg_tokens)-1]\n",
    "    \n",
    "      for index, ori_index in enumerate(trg_indexes):\n",
    "\n",
    "        trg_tk = trg_field.vocab.itos[ori_index]\n",
    "\n",
    "        if trg_tk == '<unk>':\n",
    "          attention_score2 = attention_for_alignment.squeeze(1).cpu().detach()\n",
    "\n",
    "          src_search = attention_score2[index,:].argmax()\n",
    "\n",
    "          src_search_index = src_search.numpy()\n",
    "\n",
    "          roman_token = Romanizer(src_tokens[src_search]).romanize()\n",
    "\n",
    "          trg_tokens.append(roman_token)\n",
    "\n",
    "        else:\n",
    "          trg_tokens.append(trg_tk)\n",
    "  \n",
    "    return trg_tokens[1:-1], attentions[:len(trg_tokens)-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4twt0-ydpPf"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "    \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCNZJzKseIux",
    "outputId": "d9f05b0d-6a0f-4322-e935-e102d9e1f818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 42.38\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8KH51N5cyzm"
   },
   "source": [
    "### (10) Inference \n",
    "User input 문장을 원래 순서와 역순으로 각각 넣어 inference를 진행함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_list = ['inference 문장 입력']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpDxBfMqdbC_"
   },
   "source": [
    "### (10.1) Original sentence inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse = False\n",
    "for sent in sen_list:\n",
    "  translation, attention = translate_sentence(sent, SRC, TRG, model, device, reverse = False, romanize = False)\n",
    "  translation_r, attention = translate_sentence(sent, SRC, TRG, model, device, reverse = False, romanize = True)\n",
    "  translated_text = \" \".join(translation)\n",
    "  translated_text_r = \" \".join(translation_r)\n",
    "\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted plain = {translated_text}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mocQeAFt5Ezn"
   },
   "outputs": [],
   "source": [
    "#attention display \n",
    "def display_attention(sentence, translation, attention, reverse = False):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    if reverse:\n",
    "      ax.set_xticklabels(['']+['<eos>']+[tok for tok in sentence.split(\" \")][::-1]+['<sos>'], \n",
    "                       rotation=45)\n",
    "    else: \n",
    "      ax.set_xticklabels(['']+['<sos>']+[tok for tok in sentence.split(\" \")]+['<eos>'], \n",
    "                           rotation=45)\n",
    "    ax.set_yticklabels(['']+translation)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse = False / romanize = False\n",
    "for sent in sen_list:\n",
    "  translation_r, attention = translate_sentence(sent, SRC, TRG, model, device, reverse = False, romanize = True)\n",
    "  translated_text_r = \" \".join(translation_r)\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  display_attention(sent, translation_r, attention)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrGxMSpXd2c1"
   },
   "source": [
    "### (10.2) Reversed sentence inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse = true\n",
    "for sent in sen_list:\n",
    "  translation, attention = translate_sentence(sent, SRC, TRG, model, device, reverse = True, romanize = False)\n",
    "  translation_r, attention = translate_sentence(sent, SRC, TRG, model, device, reverse = True, romanize = True)\n",
    "  translated_text = \" \".join(translation)\n",
    "  translated_text_r = \" \".join(translation_r)\n",
    "\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted plain = {translated_text}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse = True / romanize = False\n",
    "for sent in sen_list:\n",
    "  translation, attention = translate_sentence(sent, SRC, TRG, model, device, reverse = True, romanize = False)\n",
    "  translated_text = \" \".join(translation)\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted romanize = {translated_text}')\n",
    "  display_attention(sent, translation, attention, reverse = True)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymrcPeKE3Yoz"
   },
   "source": [
    "### 3.2. Convolutional Seq2seq \n",
    "Reference: https://github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzGA06qn3dZt"
   },
   "source": [
    "### (1) Field 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8YhjuRY37Zk"
   },
   "outputs": [],
   "source": [
    "#Field 정의\n",
    "#Packed padded encoder-decoder model과 같은 tokenizer 사용\n",
    "from torchtext.data import Field\n",
    "SRC_2 = Field(tokenize = tokenize_ko, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,\n",
    "            batch_first = True)\n",
    "\n",
    "TRG_2 = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYceqgtX4Jgx"
   },
   "source": [
    "### (2) TranslationDataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi27p0K94N0K"
   },
   "outputs": [],
   "source": [
    "##셔플된 데이터를 torchtext의 TranslationDataset을 통해 불러옴\n",
    "data_shuffled_2 = TranslationDataset(path = '/content/drive/MyDrive/dataset/ordered_data/', exts = ('ko_shuffle.txt', 'en_shuffle.txt'), fields = (SRC_2, TRG_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBoDyBMjD14y"
   },
   "source": [
    "### (3) Train, Validation, Test Dataset 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcJVI-nF4t41"
   },
   "outputs": [],
   "source": [
    "#train, validation, test dataset 각각 80:10:10 비율로 생성 \n",
    "train_data_2, test_data_2 = data_shuffled_2.split(split_ratio = 0.8, random_state = random.seed(SEED))\n",
    "valid_data_2, test_data_2 = test_data_2.split(split_ratio = 0.5, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IqwYpOB4t42",
    "outputId": "287decb3-445f-44d7-d08a-2f100deaf2e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 264779\n",
      "Number of validation examples: 33098\n",
      "Number of testing examples: 33097\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data_2.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data_2.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data_2.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehBCvfeRD7K5"
   },
   "source": [
    "### (4) 한국어, 영어 vocab 생성 및 사전학습 임베딩값 설정\n",
    "Reference: https://rohit-agrawal.medium.com/using-fine-tuned-gensim-word2vec-embeddings-with-torchtext-and-pytorch-17eea2883cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XV8GKl4dOb75",
    "outputId": "419bed03-7f62-47cb-f561-0f04f4308cc2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.en.vec: 6.60GB [04:24, 24.9MB/s]                            \n",
      "  0%|          | 0/2519370 [00:00<?, ?it/s]Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|█████████▉| 2518747/2519370 [04:50<00:00, 8990.86it/s]"
     ]
    }
   ],
   "source": [
    "#영어 임베딩으로 torchtext에서 제공하는 fasttext.en.300d 모델을 사용\n",
    "SRC_2.build_vocab(train_data_2, min_freq=2)\n",
    "TRG_2.build_vocab(train_data_2, vectors='fasttext.en.300d', min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHysTNZN42H4",
    "outputId": "e21795e2-1e4a-41c1-834f-4167e9bd54f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ko) vocabulary: 16179\n",
      "Unique tokens in target (en) vocabulary: 14245\n"
     ]
    }
   ],
   "source": [
    "#한국어와 영어 vocabulary 개수 \n",
    "print(f\"Unique tokens in source (ko) vocabulary: {len(SRC_2.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG_2.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fY5lZAJ842H5"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "path = '/content/drive/MyDrive/model/'\n",
    "Word2Vec_300D_token_model = KeyedVectors.load_word2vec_format(path + 'Word2Vec_300D_token.model', binary=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "8fc49ee64e1d40edbcf2204a182433b4",
      "f35827a1f0a64a4e9dac909c25145c1c",
      "2691d54c0d444a449f805eaa1cd4e38c",
      "633acbedaca84e809f798ee3e573bdae",
      "7880cedaf7764d69b90f049639675562",
      "350f96028b1744fe8bbecf2de0e51aa3",
      "910d318d71064c18bf8cbdc4de2756d9",
      "62b698f11b814127a19b3e49d496c851"
     ]
    },
    "id": "c6Qusc7S42H6",
    "outputId": "8d9ad6f1-3ddd-4cb1-89c4-b1749164cfe7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc49ee64e1d40edbcf2204a182433b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16179.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec_vectors_src_2 = []\n",
    "\n",
    "for token, idx in tqdm_notebook(SRC_2.vocab.stoi.items()):\n",
    "    if token in Word2Vec_300D_token_model.wv.vocab.keys(): #사전학습 임베딩 모델에 해당 토큰의 임베딩 값이 있을 경우 그 값을 가져옴\n",
    "        word2vec_vectors_src_2.append(torch.FloatTensor(Word2Vec_300D_token_model[token]))\n",
    "    else:\n",
    "        word2vec_vectors_src_2.append(torch.randn(300)) #사전학습 임베딩 모델에 임베딩 값이 없을 경우 랜덤으로 설정\n",
    "        \n",
    "SRC_2.vocab.set_vectors(SRC_2.vocab.stoi, word2vec_vectors_src_2, 300) #Vocab 각 토큰의 임베딩 값 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArhZ70mrEvPZ"
   },
   "source": [
    "### (5) BucketIterator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npERCc-d51_u",
    "outputId": "45cefc25-b0d0-46da-8f26-6bd3e146d31c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#BucketIterator 생성\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator_2, valid_iterator_2, test_iterator_2 = BucketIterator.splits(\n",
    "    (train_data_2, valid_data_2, test_data_2), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka8g7CD3E15B"
   },
   "source": [
    "### (6) Convoulational Sequence to Sequence Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtGLIWkBmDh_"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [0, 1, 2, 3, ..., src len - 1]\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, src len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, src len]\n",
    "        \n",
    "        #begin convolutional blocks...\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input))\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, src len]\n",
    "\n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "        \n",
    "        #...end convolutional blocks\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved = [batch size, src len, emb dim]\n",
    "        \n",
    "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKAh3HkMmOv5"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 trg_pad_idx, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        #conved = [batch size, hid dim, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved_emb = [batch size, trg len, emb dim]\n",
    "        \n",
    "        combined = (conved_emb + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, trg len, emb dim]\n",
    "                \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        \n",
    "        #energy = [batch size, trg len, src len]\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        \n",
    "        #attention = [batch size, trg len, src len]\n",
    "            \n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, emd dim]\n",
    "        \n",
    "        #convert from emb dim -> hid dim\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #apply residual connection\n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        \n",
    "        #attended_combined = [batch size, hid dim, trg len]\n",
    "        \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "            \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, trg len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = [batch size, trg len, emb dim]\n",
    "        #pos_embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, trg len]\n",
    "        \n",
    "        batch_size = conv_input.shape[0]\n",
    "        hid_dim = conv_input.shape[1]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #apply dropout\n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            #need to pad so decoder can't \"cheat\"\n",
    "            padding = torch.zeros(batch_size, \n",
    "                                  hid_dim, \n",
    "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
    "                \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
    "        \n",
    "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(padded_conv_input)\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, trg len]\n",
    "            \n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #calculate attention\n",
    "            attention, conved = self.calculate_attention(embedded, \n",
    "                                                         conved, \n",
    "                                                         encoder_conved, \n",
    "                                                         encoder_combined)\n",
    "            \n",
    "            #attention = [batch size, trg len, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "         \n",
    "        #conved = [batch size, trg len, emb dim]\n",
    "            \n",
    "        output = self.fc_out(self.dropout(conved))\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2Ip4E_KmU9U"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n",
    "           \n",
    "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
    "        #encoder_conved is output from final encoder conv. block\n",
    "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
    "        #  positional embeddings \n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "            \n",
    "        #encoder_conved = [batch size, src len, emb dim]\n",
    "        #encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #calculate predictions of next words\n",
    "        #output is a batch of predictions for each word in the trg sentence\n",
    "        #attention a batch of attention scores across the src sentence for \n",
    "        #  each word in the trg sentence\n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #attention = [batch size, trg len - 1, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM4k1_ugE4w0"
   },
   "source": [
    "### (7) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTG4rdXm6OhI"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC_2.vocab)\n",
    "OUTPUT_DIM = len(TRG_2.vocab)\n",
    "EMB_DIM = 300\n",
    "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
    "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
    "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
    "ENC_KERNEL_SIZE = 3 # must be odd!\n",
    "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "TRG_PAD_IDX_2 = TRG_2.vocab.stoi[TRG_2.pad_token]\n",
    "    \n",
    "enc_2 = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
    "dec_2 = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX_2, device)\n",
    "\n",
    "model_2 = Seq2Seq(enc_2, dec_2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HdoAOPj6OhJ",
    "outputId": "dc455fdb-5971-45b1-cfc5-c3b0c55a7d25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16179, 300)\n",
       "    (pos_embedding): Embedding(100, 300)\n",
       "    (emb2hid): Linear(in_features=300, out_features=512, bias=True)\n",
       "    (hid2emb): Linear(in_features=512, out_features=300, bias=True)\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (8): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (9): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(14245, 300)\n",
       "    (pos_embedding): Embedding(100, 300)\n",
       "    (emb2hid): Linear(in_features=300, out_features=512, bias=True)\n",
       "    (hid2emb): Linear(in_features=512, out_features=300, bias=True)\n",
       "    (attn_hid2emb): Linear(in_features=512, out_features=300, bias=True)\n",
       "    (attn_emb2hid): Linear(in_features=300, out_features=512, bias=True)\n",
       "    (fc_out): Linear(in_features=300, out_features=14245, bias=True)\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (8): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (9): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model_2.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcgCeeRP6OhL",
    "outputId": "15ef4d6b-6c37-4fe7-9674-87d115ba050b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.1093, -0.1233, -0.2521,  ...,  0.3467,  0.1614,  0.1974],\n",
       "        [ 0.0728, -0.2033, -0.0522,  ...,  0.2844,  0.0839, -0.2510],\n",
       "        [-0.0395, -0.2716, -0.0760,  ...,  0.5530, -0.0286,  0.1793]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings_src_2 = SRC_2.vocab.vectors\n",
    "model_2.encoder.tok_embedding.weight.data.copy_(pretrained_embeddings_src_2)\n",
    "\n",
    "pretrained_embeddings_trg_2 = TRG_2.vocab.vectors\n",
    "model_2.decoder.tok_embedding.weight.data.copy_(pretrained_embeddings_trg_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BfkfbpyY6OhM",
    "outputId": "27578a5d-acf4-4828-dbe6-0266628582a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 45,876,741 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model_2):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2zLO7MH6OhM"
   },
   "outputs": [],
   "source": [
    "optimizer_2 = optim.Adam(model_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIgHvp7H6OhN"
   },
   "outputs": [],
   "source": [
    "TRG_PAD_IDX_2 = TRG_2.vocab.stoi[TRG_2.pad_token]\n",
    "\n",
    "criterion_2 = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdUwwZ-e_bVx"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "454N30CQ_cRw"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6RkRXp86OhP"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfRANWuo_kSw",
    "outputId": "a449d336-2099-4f24-f904-6f78574752d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 9m 28s\n",
      "\tTrain Loss: 3.791 | Train PPL:  44.291\n",
      "\t Val. Loss: 2.039 |  Val. PPL:   7.683\n",
      "Epoch: 02 | Time: 9m 39s\n",
      "\tTrain Loss: 2.088 | Train PPL:   8.071\n",
      "\t Val. Loss: 1.626 |  Val. PPL:   5.083\n",
      "Epoch: 03 | Time: 9m 37s\n",
      "\tTrain Loss: 1.802 | Train PPL:   6.061\n",
      "\t Val. Loss: 1.595 |  Val. PPL:   4.927\n",
      "Epoch: 04 | Time: 9m 37s\n",
      "\tTrain Loss: 2.220 | Train PPL:   9.205\n",
      "\t Val. Loss: 2.788 |  Val. PPL:  16.247\n",
      "Epoch: 05 | Time: 9m 38s\n",
      "\tTrain Loss: 1.620 | Train PPL:   5.053\n",
      "\t Val. Loss: 1.350 |  Val. PPL:   3.858\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model_2, train_iterator_2, optimizer_2, criterion_2, CLIP)\n",
    "    valid_loss = evaluate(model_2, valid_iterator_2, criterion_2)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_2.state_dict(), '/content/drive/MyDrive/dataset/conv-seq2seq.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Oby3rSC6OhQ",
    "outputId": "e2ceba57-525b-405e-c2f9-937a2b68afee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 1.336 | Test PPL:   3.803 |\n"
     ]
    }
   ],
   "source": [
    "model_2.load_state_dict(torch.load('/content/drive/MyDrive/dataset/conv-seq2seq.pt'))\n",
    "\n",
    "test_loss = evaluate(model_2, test_iterator_2, criterion_2)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziIZRJ38ISQV"
   },
   "source": [
    "### (9) BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0i0BJY9DYBF"
   },
   "outputs": [],
   "source": [
    "#input이 tokenize되지 않은 string일 경우 tokenize 후 번역, 이미 tokenize된 list일 경우 그대로 번역\n",
    "#<unk> token을 로마자화할 수 있는 옵션이 있음  \n",
    "def translate_sentence_conv(sentence, src_field, trg_field, model, device, max_len = 50, reverse = False, romanize=False):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(sentence, list):\n",
    "      tokens = sentence\n",
    "    elif isinstance(sentence, str):                      # input이 tokenize되어있지 않을 경우 tokenize 시행 \n",
    "      if reverse == False:                               \n",
    "        tokens = [tok for tok in sentence.split(\" \")]\n",
    "      elif reverse == True:                              # input 문장을 역순으로 넣어 번역할 경우 \n",
    "        tokens = [tok for tok in sentence.split(\" \")][::-1]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tokens = [src_field.vocab.itos[i] for i in src_indexes]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_conved, encoder_combined = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    if romanize == False:\n",
    "      trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    elif romanize == True: \n",
    "      trg_tokens = []\n",
    "\n",
    "      attention_for_alignment = attention\n",
    "    \n",
    "      for index, ori_index in enumerate(trg_indexes):\n",
    "\n",
    "        trg_tk = trg_field.vocab.itos[ori_index]\n",
    "\n",
    "        if trg_tk == '<unk>':\n",
    "          attention_score2 = attention_for_alignment.squeeze(0).cpu().detach()\n",
    "\n",
    "          src_search = attention_score2[index,:].argmax()\n",
    "\n",
    "          src_search_index = src_search.numpy()\n",
    "\n",
    "          roman_token = Romanizer(src_tokens[src_search]).romanize()\n",
    "\n",
    "          trg_tokens.append(roman_token)\n",
    "\n",
    "        else:\n",
    "          trg_tokens.append(trg_tk)\n",
    "    \n",
    "    return trg_tokens[1:-1], attention.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZxJf1LXUtYX"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu_conv(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence_conv(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXj-oXrHJo7d",
    "outputId": "e1f3cc7d-c34b-4dfa-f8e5-5f4a22ea4ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 38.11\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu_conv(test_data_2, SRC_2, TRG_2, model_2, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAQoHataXjBA"
   },
   "source": [
    "### (10) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKqASp_r9LP7"
   },
   "outputs": [],
   "source": [
    "sen_list = ['inference 문장 입력']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a_LlTu_nzs9"
   },
   "source": [
    "### (10.1) Original sentence inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(no reverse) source sentence에 대해 영어로 번역 / 영어로 번역 + <unk> 토큰 로마자화 결과 각각 출력 \n",
    "for sent in sen_list:\n",
    "  translation, attention = translate_sentence_conv(sent, SRC_2, TRG_2, model_2, device, reverse = False, romanize = False)\n",
    "  translation_r, attention_r = translate_sentence_conv(sent, SRC_2, TRG_2, model_2, device, reverse = False, romanize = True )\n",
    "  translated_text = \" \".join(translation)\n",
    "  translated_text_r = \" \".join(translation_r)\n",
    "\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted plain = {translated_text}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gBlM4qcBohh"
   },
   "outputs": [],
   "source": [
    "def display_attention_conv(sentence, translation, attention, reverse = False):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(0).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=12)\n",
    "    if reverse:\n",
    "      ax.set_xticklabels(['']+['<eos>']+[tok for tok in sentence.split(\" \")][::-1]+['<sos>'], \n",
    "                       rotation=45)\n",
    "    else:\n",
    "      ax.set_xticklabels(['']+['<sos>']+[tok for tok in sentence.split(\" \")]+['<eos>'], \n",
    "                           rotation=45)\n",
    "    ax.set_yticklabels(['']+translation)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(no reverse)\n",
    "#source sent에 대해 영어로 번역 + <unk> 토큰 로마자화 결과 출력\n",
    "#두 문장 간의 attention display도 함께출력\n",
    "\n",
    "for sent in sen_list:\n",
    "  translation_r, attention = translate_sentence_conv(sent, SRC_2, TRG_2, model_2, device, romanize = True)\n",
    "  translated_text_r = \" \".join(translation_r)\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  display_attention_conv(sent, translation_r, attention)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VORDsj6snsK4"
   },
   "source": [
    "### (10.2) Reversed sentence inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source sentence에 대해 영어로 번역 / 영어로 번역 + <unk> 토큰 로마자화 결과 각각 출력 \n",
    "for sent in sen_list:\n",
    "  translation, attention = translate_sentence_conv(sent, SRC_2, TRG_2, model_2, device, reverse = True, romanize = False)\n",
    "  translation_r, attention_r = translate_sentence_conv(sent, SRC_2, TRG_2, model_2, device, reverse = True, romanize = True)\n",
    "  translated_text = \" \".join(translation)\n",
    "  translated_text_r = \" \".join(translation_r)\n",
    "\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted plain = {translated_text}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source sent에 대해 영어로 번역 + <unk> 토큰 로마자화 결과 출력\n",
    "#두 문장 간의 attention display도 함께출력\n",
    "\n",
    "for sent in sen_list:\n",
    "  translation_r, attention = translate_sentence_conv(sent, SRC_2, TRG_2, model_2, device, reverse = True, romanize = True)\n",
    "  translated_text_r = \" \".join(translation_r)\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  display_attention_conv(sent, translation_r, attention, reverse = True)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfx-c8Vb9I3S"
   },
   "source": [
    "### 3.3 Transformers\n",
    "Reference: https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJmcIf-E-D-V"
   },
   "source": [
    "### (1) Field 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gw4i_7nO-D-d",
    "outputId": "9a72bd83-9358-4f79-fd5d-cdb62bfcd21a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#Field 정의\n",
    "#앞의 모델들과 동일한 토크나이저 사용\n",
    "SRC_3 = Field(tokenize = tokenize_ko, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG_3 = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC0kiW-1-D-f"
   },
   "source": [
    "### (2) TranslationDataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7l1fHWi-D-g",
    "outputId": "831998b5-d3c5-47a1-deaf-e8dd782314d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#셔플된 데이터를 torchtext의 TranslationDataset을 통해 불러옴\n",
    "\n",
    "data_shuffled_3 = TranslationDataset(path = '/content/drive/MyDrive/dataset/ordered_data/', exts = ('ko_shuffle.txt', 'en_shuffle.txt'), fields = (SRC_3, TRG_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQaVOLuv-D-h"
   },
   "source": [
    "### (3) Train, Validation, Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNJaX73G-D-h"
   },
   "outputs": [],
   "source": [
    "#train, validation, test dataset 각각 80:10:10 비율로 생성 \n",
    "train_data_3, test_data_3 = data_shuffled_3.split(split_ratio = 0.8, random_state = random.seed(SEED))\n",
    "valid_data_3, test_data_3 = test_data_3.split(split_ratio = 0.5, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxVvhwbm-D-i",
    "outputId": "00a88dde-ee15-4344-8e32-b6587e42cfa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 264779\n",
      "Number of validation examples: 33098\n",
      "Number of testing examples: 33097\n"
     ]
    }
   ],
   "source": [
    "#train, validation, test data 개수 확인 \n",
    "print(f\"Number of training examples: {len(train_data_3.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data_3.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data_3.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kvCDZYY-D-j"
   },
   "source": [
    "### (4) 한국어, 영어 vocab 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHbXMFO_JzhP"
   },
   "outputs": [],
   "source": [
    "#transformer 구현 시에는 word embedding을 사용하지 않고 초기에 랜덤으로 벡터 설정\n",
    "SRC_3.build_vocab(train_data_3, min_freq=2)\n",
    "TRG_3.build_vocab(train_data_3, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96GAXlXH-D-k",
    "outputId": "29a42938-0644-46ee-eaa0-7d400388d3eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ko) vocabulary: 16179\n",
      "Unique tokens in target (en) vocabulary: 14245\n"
     ]
    }
   ],
   "source": [
    "#한국어와 영어 vocabulary 개수 \n",
    "print(f\"Unique tokens in source (ko) vocabulary: {len(SRC_3.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG_3.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyZAONgj-D-k"
   },
   "source": [
    "### (5) BucketIterator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3O-iLP5-D-l"
   },
   "outputs": [],
   "source": [
    "#device 정의 \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPbQ91tk-D-l",
    "outputId": "071c66c9-d341-4048-c64c-e84b96c724d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#BucketIterator 생성 \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator_3, valid_iterator_3, test_iterator_3 = BucketIterator.splits(\n",
    "    (train_data_3, valid_data_3, test_data_3), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otRmDFKm-D-m"
   },
   "source": [
    "### (6) Transformer Model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaU0qRxK-D-n"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JDXyMoh-D-n"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len] \n",
    "                \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-LZZJ2q-D-o"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MpzUE95-D-p"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PVAwbPcHKmJ"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "                            \n",
    "        #pos = [batch size, trg len]\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "                \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fTW-ptgHOeY"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0IZPumBHPi6"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvLZrxOb-D-r"
   },
   "source": [
    "### (7) Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNd4daYK-D-s"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC_3.vocab)\n",
    "OUTPUT_DIM = len(TRG_3.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc_3 = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec_3 = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3JNex7DKafQ"
   },
   "outputs": [],
   "source": [
    "SRC_PAD_IDX_3 = SRC_3.vocab.stoi[SRC_3.pad_token]\n",
    "TRG_PAD_IDX_3 = TRG_3.vocab.stoi[TRG_3.pad_token]\n",
    "\n",
    "model_3 = Seq2Seq(enc_3, dec_3, SRC_PAD_IDX_3, TRG_PAD_IDX_3, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1yO-pKgKbW9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogi4bs8N-D-t",
    "outputId": "c6530572-f991-4358-d786-a9e742f2c8d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16179, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(14245, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=14245, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nMyYyGE-D-t",
    "outputId": "1fc34c48-c5eb-4d2c-f6af-a7bcebf2c9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 15,454,373 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "#모델에 쓰인 파라미터 개수 \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model_3):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndkhLLPu-D-u"
   },
   "outputs": [],
   "source": [
    "#Adam optimizer 사용 \n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer_3 = torch.optim.Adam(model_3.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWoJMg8E-D-u"
   },
   "outputs": [],
   "source": [
    "#Cross entropy loss 함수 사용 \n",
    "criterion_3 = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX_3) #<pad> token은 loss 계산 시 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6Fj_xXT-D-u"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8kNefcw-D-u"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJ80dPYk-D-x"
   },
   "outputs": [],
   "source": [
    "#epoch 당 소요시간 계산 \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6X0QWQ5-D-x",
    "outputId": "69f70ba7-3aa1-473c-8384-a33952e8a4f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 3m 21s\n",
      "\tTrain Loss: 2.580 | Train PPL:  13.202\n",
      "\t Val. Loss: 1.580 |  Val. PPL:   4.854\n",
      "Epoch: 02 | Time: 3m 25s\n",
      "\tTrain Loss: 1.530 | Train PPL:   4.620\n",
      "\t Val. Loss: 1.272 |  Val. PPL:   3.566\n",
      "Epoch: 03 | Time: 3m 24s\n",
      "\tTrain Loss: 1.233 | Train PPL:   3.433\n",
      "\t Val. Loss: 1.136 |  Val. PPL:   3.115\n",
      "Epoch: 04 | Time: 3m 24s\n",
      "\tTrain Loss: 1.065 | Train PPL:   2.901\n",
      "\t Val. Loss: 1.066 |  Val. PPL:   2.903\n",
      "Epoch: 05 | Time: 3m 25s\n",
      "\tTrain Loss: 0.954 | Train PPL:   2.596\n",
      "\t Val. Loss: 1.010 |  Val. PPL:   2.746\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model_3, train_iterator_3, optimizer_3, criterion_3, CLIP)\n",
    "    valid_loss = evaluate(model_3, valid_iterator_3, criterion_3)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_3.state_dict(), '/content/drive/MyDrive/dataset/transformer-word2vec-fasttext-ordered-data.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaQtkIk9-D-y",
    "outputId": "7a66d77e-9fb8-4de7-f700-af87e1b7cb9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 1.019 | Test PPL:   2.770 |\n"
     ]
    }
   ],
   "source": [
    "model_3.load_state_dict(torch.load('/content/drive/MyDrive/dataset/transformer-word2vec-fasttext-ordered-data.pt'))\n",
    "\n",
    "test_loss = evaluate(model_3, test_iterator_3, criterion_3)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnOgX7r7-D-z"
   },
   "source": [
    "### (9) BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC1MazO0DddJ"
   },
   "outputs": [],
   "source": [
    "def translate_sentence_transformer(sentence, src_field, trg_field, model, device, max_len = 50, reverse = False, romanize = False):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, list):\n",
    "        tokens = sentence\n",
    "    elif isinstance(sentence, str):\n",
    "        if reverse == True:\n",
    "          tokens = [tok for tok in sentence.split(\" \")][::-1]\n",
    "        elif reverse == False:\n",
    "          tokens = [tok for tok in sentence.split(\" \")]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tokens = [src_field.vocab.itos[i] for i in src_indexes]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    if romanize == False: \n",
    "        trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    elif romanize == True:\n",
    "        trg_tokens = []\n",
    "\n",
    "        attention_for_alignment = attention\n",
    "    \n",
    "        for index, ori_index in enumerate(trg_indexes):\n",
    "\n",
    "          trg_tk = trg_field.vocab.itos[ori_index]\n",
    "\n",
    "          if trg_tk == '<unk>':\n",
    "              attention_score2 = attention_for_alignment.squeeze(0).cpu().detach()\n",
    "\n",
    "              attention_mean = attention_score2.mean(0)\n",
    "\n",
    "              src_search = attention_mean[index,:].argmax()\n",
    "\n",
    "              src_search_index = src_search.numpy()\n",
    "\n",
    "              for_roman = src_tokens[src_search_index]\n",
    "\n",
    "              clean_fr = re.compile('[가-힣]+').findall(for_roman) #'ㄴ다' 같이 자/모 하나만 있는 것 제거\n",
    "\n",
    "              roman_token = Romanizer(''.join(clean_fr)).romanize()\n",
    "\n",
    "              trg_tokens.append(roman_token)\n",
    "\n",
    "          else:\n",
    "              trg_tokens.append(trg_tk)   \n",
    "    \n",
    "    return trg_tokens[1:-1], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__0yCG4Z-D-0"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu_transformer(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence_transformer(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkpmV81K-D-0",
    "outputId": "5ca3a3f5-7384-4a2b-8801-352491e78456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 43.92\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu_transformer(test_data_3, SRC_3, TRG_3, model_3, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uJt2NWQ-D-1"
   },
   "source": [
    "### (10) Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Qe5afJZ-D-1"
   },
   "outputs": [],
   "source": [
    "sen_list = ['inference 문장 입력']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rg-L9m4GX7g"
   },
   "source": [
    "### (10.1) Original sentence inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(no reverse) source sentence에 대해 영어로 번역 / 영어로 번역 + <unk> 토큰 로마자화 결과 각각 출력 \n",
    "for sent in sen_list:\n",
    "  translation, attention = translate_sentence_transformer(sent, SRC_3, TRG_3, model_3, device, reverse = False, romanize = False)\n",
    "  translation_r, attention_r = translate_sentence_transformer(sent, SRC_3, TRG_3, model_3, device, reverse = False, romanize = True)\n",
    "  translated_text = \" \".join(translation)\n",
    "  translated_text_r = \" \".join(translation_r)\n",
    "\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted plain = {translated_text}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uA2gy7bo-D-3"
   },
   "outputs": [],
   "source": [
    "def display_attention_transformer(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2, reverse = False):\n",
    "    \n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        if reverse: \n",
    "          ax.set_xticklabels(['']+['<eos>']+[tok for tok in sentence.split(\" \")][::-1]+['<sos>'], \n",
    "                           rotation=45)\n",
    "        else:\n",
    "          ax.set_xticklabels(['']+['<sos>']+[tok for tok in sentence.split(\" \")]+['<eos>'], \n",
    "                           rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(no reverse)\n",
    "#source sent에 대해 영어로 번역 + <unk> 토큰 로마자화 결과 출력\n",
    "#두 문장 간의 attention display도 함께출력\n",
    "\n",
    "for sent in sen_list:\n",
    "  translation, attention = translate_sentence_transformer(sent, SRC_3, TRG_3, model_3, device, reverse = False, romanize = True)\n",
    "  translated_text_r = \" \".join(translation)\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  display_attention_transformer(sent, translation, attention)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjzuXpGbGxAZ"
   },
   "source": [
    "### (10.2) Reversed sentence inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference 문장 역순으로 입력\n",
    "#source sentence에 대해 영어로 번역 / 영어로 번역 + <unk> 토큰 로마자화 결과 각각 출력 \n",
    "for sent in sen_list:\n",
    "  translation, attention = translate_sentence_transformer(sent, SRC_3, TRG_3, model_3, device, reverse = True, romanize = False)\n",
    "  translation_r, attention_r = translate_sentence_transformer(sent, SRC_3, TRG_3, model_3, device, reverse = True, romanize = True)\n",
    "  translated_text = \" \".join(translation)\n",
    "  translated_text_r = \" \".join(translation_r)\n",
    "\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted plain = {translated_text}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source sent에 대해 영어로 번역 + <unk> 토큰 로마자화 결과 출력\n",
    "#두 문장 간의 attention display도 함께출력\n",
    "\n",
    "for sent in sen_list:\n",
    "  translation, attention = translate_sentence_transformer(sent, SRC_3, TRG_3, model_3, device, reverse = True, romanize = True)\n",
    "  translated_text_r = \" \".join(translation)\n",
    "  print(f'source sent = {sent}')\n",
    "  print(f'predicted romanize = {translated_text_r}')\n",
    "  display_attention_transformer(sent, translation, attention, reverse = True)\n",
    "  print('\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "HR0aBWzetM7M",
    "pK79VGjEvD1f",
    "c2RCMjtFWw23",
    "D88HRMw_YU2a",
    "nDCbo6LFZs1z",
    "xliZr485aB42"
   ],
   "name": "FinalProject_3_models_combined+ordered_data.ipynb",
   "provenance": [
    {
     "file_id": "1DH_7_XGyPUnXcXD97vcTj26ut6hdLtxc",
     "timestamp": 1607858611969
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "39c2244510d64000803b98cb800c636e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64a2cd866508454a84cd6d416ac3e0b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7480b0b20d554134948ee2e3e9512f87",
      "placeholder": "​",
      "style": "IPY_MODEL_39c2244510d64000803b98cb800c636e",
      "value": " 16179/16179 [00:00&lt;00:00, 71264.07it/s]"
     }
    },
    "680afb4881d54c6ab138e98ad45a82e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b48d528f62e74f51a359232518198b3b",
      "max": 16179,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_882acbc01eed4a10aeb7d4ad7866b527",
      "value": 16179
     }
    },
    "7480b0b20d554134948ee2e3e9512f87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "882acbc01eed4a10aeb7d4ad7866b527": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "912fd8ac30e64ca7904a0d847952af40": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b48d528f62e74f51a359232518198b3b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df37647370c24c62b9639aeba45d7c2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_680afb4881d54c6ab138e98ad45a82e3",
       "IPY_MODEL_64a2cd866508454a84cd6d416ac3e0b4"
      ],
      "layout": "IPY_MODEL_912fd8ac30e64ca7904a0d847952af40"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
